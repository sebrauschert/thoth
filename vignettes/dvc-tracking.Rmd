---
title: "Data Version Control with thoth"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Data Version Control with thoth}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE
)
```

```{r setup}
library(thoth)
```

## Introduction

Data Version Control (DVC) is a powerful tool for managing data science workflows. While DVC is written in Python, `thoth` provides seamless integration with R, allowing you to:

1. Track large data files without storing them in Git
2. Create reproducible pipelines
3. Track metrics and plots
4. Share data across team members

This guide will walk you through setting up DVC and using it effectively with `thoth`.

## Prerequisites

Before starting, ensure you have:

1. DVC installed:
   ```bash
   pip install dvc
   ```

2. Git initialized in your project:
   ```bash
   git init
   ```

3. thoth installed:
   ```r
   remotes::install_github("sebrauschert/thoth")
   ```

## Project Setup

When you create a new analytics project with `thoth`, DVC is automatically set up:

```r
library(thoth)
create_analytics_project(
  "my_analysis",
  use_dvc = TRUE  # DVC is enabled by default
)
```

This:
- Initializes DVC
- Creates data directories tracked by DVC
- Sets up appropriate .gitignore rules
- Creates a basic DVC pipeline structure

## Simple Data Tracking

### Tracking CSV Files

The simplest way to track data files is using the DVC-aware write functions:

```r
# Track a CSV file
data |> write_csv_dvc(
  "data/processed/results.csv",
  message = "Add processed results"
)

# Track with additional readr::write_csv parameters
data |> write_csv_dvc(
  "data/processed/results.csv",
  message = "Add processed results",
  na = "NA",
  quote = "needed"
)
```

### Tracking R Objects

For R objects, use `write_rds_dvc`:

```r
# Save and track a model
model |> write_rds_dvc(
  "models/rf_model.rds",
  message = "Save trained random forest model"
)
```

### Manual Tracking

You can also track files after they've been written:

```r
# Write file normally
readr::write_csv(data, "data/processed/results.csv")

# Track with DVC
dvc_track("data/processed/results.csv", "Add processed results")
```

## Creating DVC Pipelines

DVC pipelines help make your analysis reproducible by tracking:
- Input dependencies
- Output files
- Parameters
- Metrics and plots

### Basic Pipeline Stage

```r
# Create a data processing stage
data |> write_csv_dvc(
  "data/processed/results.csv",
  stage_name = "process_data",
  deps = "data/raw/input.csv",
  params = list(threshold = 0.5)
)
```

### Tracking Metrics

Track evaluation metrics automatically:

```r
metrics |> write_csv_dvc(
  "metrics/evaluation.csv",
  stage_name = "evaluate_model",
  deps = c("models/rf_model.rds", "data/test/test.csv"),
  metrics = TRUE  # Mark as DVC metrics
)
```

### Tracking Plots

Track visualization files:

```r
# Save plot data with DVC tracking
plot_data |> write_csv_dvc(
  "plots/feature_importance.csv",
  stage_name = "create_plots",
  deps = "models/rf_model.rds",
  plots = TRUE  # Mark as DVC plot
)
```

## Complete Pipeline Example

Here's a complete example of a machine learning pipeline:

```r
library(thoth)

# 1. Data Preprocessing
raw_data |> write_csv_dvc(
  "data/processed/preprocessed.csv",
  stage_name = "preprocess",
  deps = "data/raw/input.csv",
  params = list(
    remove_na = TRUE,
    normalize = TRUE
  )
)

# 2. Feature Engineering
features |> write_csv_dvc(
  "data/processed/features.csv",
  stage_name = "feature_engineering",
  deps = "data/processed/preprocessed.csv",
  params = list(
    n_components = 10
  )
)

# 3. Model Training
model |> write_rds_dvc(
  "models/rf_model.rds",
  stage_name = "train_model",
  deps = "data/processed/features.csv",
  params = list(
    ntree = 500,
    mtry = 3
  )
)

# 4. Model Evaluation
metrics |> write_csv_dvc(
  "metrics/evaluation.csv",
  stage_name = "evaluate",
  deps = c(
    "models/rf_model.rds",
    "data/processed/features.csv"
  ),
  metrics = TRUE
)

# 5. Create Plots
plot_data |> write_csv_dvc(
  "plots/performance.csv",
  stage_name = "visualize",
  deps = "metrics/evaluation.csv",
  plots = TRUE
)
```

## Working with Remote Storage

DVC supports various remote storage options (S3, GCS, etc.). Once configured, you can:

```r
# Pull data from remote
dvc_pull()

# Push data to remote
dvc_push()

# Pull specific files
dvc_pull("data/processed/features.csv")
```

## Best Practices

1. **Organize Data**:
   - Keep raw data in `data/raw/`
   - Store processed data in `data/processed/`
   - Save models in `models/`
   - Keep metrics in `metrics/`
   - Store plots in `plots/`

2. **Use Meaningful Stage Names**:
   - Make stage names descriptive
   - Use consistent naming conventions
   - Include the purpose in the name

3. **Track Dependencies**:
   - Always specify input dependencies
   - Include R scripts as dependencies
   - Track configuration files

4. **Use Parameters**:
   - Track important hyperparameters
   - Include preprocessing parameters
   - Document parameter choices

5. **Commit Regularly**:
   - Commit .dvc files to Git
   - Use meaningful commit messages
   - Push data to remote storage

## Troubleshooting

Common issues and solutions:

1. **DVC not initialized**:
   ```r
   # Initialize DVC manually
   system2("dvc", "init")
   ```

2. **File not tracked**:
   ```r
   # Check if file is tracked
   system2("dvc", c("status", "path/to/file"))
   ```

3. **Pipeline not updating**:
   ```r
   # Force pipeline stage rerun
   system2("dvc", c("repro", "--force"))
   ```

## Conclusion

DVC integration in `thoth` provides a seamless way to:
- Track large data files
- Create reproducible pipelines
- Share data across team members
- Monitor experiments
- Maintain data science workflows

By following these practices, you can create reproducible analyses that are easy to share and collaborate on. 